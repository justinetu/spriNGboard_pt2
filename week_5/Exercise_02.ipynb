{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fc57c7-3fb9-470a-b816-a0882e8695bb",
   "metadata": {},
   "source": [
    "# Exercise 02: SVD/PCA on MNIST Digits\n",
    "\n",
    "In this exercise,\n",
    "you'll explore the use of Singular Value Decomposition (SVD)\n",
    "to perform Principle Component Analysis (PCA)\n",
    "on the MNIST handwritten digits dataset.\n",
    "\n",
    "As usual, let's start by installing the needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e446eb3-05c0-4ef4-ab2a-b1b228877f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (24.0)\n",
      "Requirement already satisfied: scikit-learn in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: matplotlib in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/justin/.pyenv/versions/3.10.13/envs/sb_kev_wk5/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install -U scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a807960-62e0-4c4f-9876-101574973749",
   "metadata": {},
   "source": [
    "Now let's load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a2df5a-eecd-4640-92ce-9def4eec7c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=20, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, dataset.images, dataset.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"%i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29666a9-1dbf-4f59-82b5-5aee14581d52",
   "metadata": {},
   "source": [
    "We have 1797 examples of 8x8 pixel images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a351269e-2947-420a-a6e4-f12419510ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db27750-719d-4239-9511-3dc2a0b0734d",
   "metadata": {},
   "source": [
    "**How many dimensions does each image have?**\n",
    "\n",
    "(Hint #1: The answer isn't 2.)\n",
    "\n",
    "(Hint #2: How many numbers does it take to represent one image?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caa989-5b98-4da0-9c9f-278c09380551",
   "metadata": {},
   "source": [
    "Each image has 3 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef953e4-a81a-4f23-93d0-81a509b94181",
   "metadata": {},
   "source": [
    "We're going to \"flatten\" the 8x8 pixel images into 64 pixel vectors to aid our processing.\n",
    "Note the transpose (`.T`); each image vector is now a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bd059c-09db-489f-a225-28042129dd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1797)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_samples = len(dataset.images)\n",
    "data = dataset.images.reshape((N_samples, 64)).T\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361bab3-37c4-4551-aefa-57f0f63bcbd5",
   "metadata": {},
   "source": [
    "Let's take a compare the tranpose of the first data vector\n",
    "(displayed as a row instead of a column)\n",
    "to the first 8x8 dataset image.\n",
    "Make sure you understand what you're looking at!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff00517-4af1-411c-9fa8-d1cdc4909343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[:,0]=\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "dataset.images[0]=\n",
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print('data[:,0]=\\n{}'.format(data[:,0]))\n",
    "print('dataset.images[0]=\\n{}'.format(dataset.images[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84882ea0-a490-4499-888f-b262eb2de74e",
   "metadata": {},
   "source": [
    "**Why do we need to normalize our data before performing SVD?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6fd59-53fb-41fc-882f-7d7981d6d960",
   "metadata": {},
   "source": [
    "We need to normalize our data before performing SVD because it helps SVD work better because it reduces effects of outliers. Additionally, normalization puts everything on the same scale so that a feature does not dominate the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce08cc5-846d-4e6b-97c5-8421e7e83cbc",
   "metadata": {},
   "source": [
    "Let's go ahead and normalize.  We need to keep the mean and stddev info for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c35a6d6e-db06-4b9b-adaf-2cdc8eaa0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = np.mean(data, axis=1).reshape((64,1))\n",
    "#print('data_mean = \\n{}'.format(data_mean))\n",
    "\n",
    "data_stddev = np.std(data, axis=1).reshape((64,1))\n",
    "data_stddev[data_stddev == 0] = 0.001\n",
    "#print('data_stddev = \\n{}'.format(data_stddev))\n",
    "\n",
    "data_normalized = (data - data_mean)/data_stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c5a31-b933-4d39-ad09-145c8589354d",
   "metadata": {},
   "source": [
    "Inspect the following code and its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265d8c72-4c11-469d-b046-0a82f32632ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00 -2.56086502e-16 -2.34771702e-16 -3.05326777e-16\n",
      "  1.39689163e-16 -3.16941798e-16 -9.59598443e-16  6.06946633e-16\n",
      " -4.62368967e-16  7.55084971e-17 -5.33796713e-17  5.71483749e-17\n",
      "  4.52862258e-17 -1.49512505e-17  8.96580775e-16 -2.17719863e-16\n",
      " -6.87973761e-16 -7.70421876e-17  2.59978769e-16  2.32547550e-16\n",
      "  1.05878949e-16 -1.40863021e-16 -1.15408826e-16 -2.26477465e-16\n",
      "  4.51109193e-16  3.79341646e-17 -7.71657517e-17 -9.60092699e-17\n",
      " -8.72980041e-17 -1.50624582e-16 -1.05597840e-15 -2.52140175e-16\n",
      "  0.00000000e+00  3.42148865e-16  1.21710593e-16 -1.07871419e-16\n",
      " -2.00606242e-16 -1.09354188e-17 -7.12964591e-17  0.00000000e+00\n",
      "  3.00716294e-16 -1.74657790e-16  4.03436636e-16  1.92111213e-16\n",
      "  2.61337974e-17  5.38739275e-17  4.71396866e-17  8.63759074e-16\n",
      "  2.62048467e-16  3.33128689e-16  8.36528645e-17  1.98775949e-16\n",
      "  4.61017485e-16  5.01670059e-17  4.71582212e-16 -4.92032063e-16\n",
      "  2.25353805e-16  3.44125890e-16  5.95269830e-17 -6.11460582e-16\n",
      " -5.19092591e-16  1.69529882e-16  1.78797186e-16 -6.83896148e-16]\n",
      "[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(data_normalized, axis=1))\n",
    "print(np.std(data_normalized, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03be00e-3a53-4c20-8c05-cde25df4698f",
   "metadata": {},
   "source": [
    "**Do these values make sense?  Why or why not?  Do you see anything unexpected?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed6fb2-0cc1-4913-a3e5-357178d0ce8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ed433f0-5700-4f4b-81f8-e505fe56a0df",
   "metadata": {},
   "source": [
    "Now that we have normalized data,\n",
    "we can construct the covariance matrix.\n",
    "\n",
    "**What shape do you expect the covariance matrix to have?  Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef3b74-4ae6-4efa-a351-f0b6b1805e25",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d1456-61fe-4f23-9fe0-81c8e343fd9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C = np.dot(data_normalized,data_normalized.T)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b5f28-fcc9-4ecc-8b22-78489f46a4ed",
   "metadata": {},
   "source": [
    "Before you run the code below,\n",
    "stop and think about what we're doing:\n",
    "we want to reduce the dimensionality of the MNIST dataset\n",
    "to something more \"manageable\".\n",
    "\n",
    "**What is the minimum number of dimensions we can reasonably expect for this dataset?  Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d357a-af02-448d-ae9b-f4c05f224841",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc2acf-7527-456a-a430-a316a4c36e8c",
   "metadata": {},
   "source": [
    "Let's perform SVD on the covariance matrix,\n",
    "and inspect the diagonal, `S`, of the singular value matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151510b-4ef0-42a2-a1d3-b9f1d822cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,Vh = np.linalg.svd(C)\n",
    "print('S = \\n{}'.format(S))\n",
    "plt.plot(S)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba442f-5dfd-4310-850e-72f2592970df",
   "metadata": {},
   "source": [
    "**Based on the values and scree plot above, how many meaningful dimensions does this MNIST data have?  How did you come to that conclusion?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b0130-acc6-4c37-82e8-c63d7d3fe7c6",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665bda0-2aa5-4414-b5b2-f0a369a4eba2",
   "metadata": {},
   "source": [
    "**Based on the values and scree plot above, how many dimensions can we *definitely* get rid of?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa55fb9-b52b-48f4-a1a1-069da190449b",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1469f3c-ae00-4cf9-b3ac-7128c251ac8e",
   "metadata": {},
   "source": [
    "**What do the dimensions we can eliminate say about the input data values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24fdd3c-e63c-4475-8cc9-efb12b2eb6f9",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9e3cb-c8f3-4cde-9bb9-fd7572005a11",
   "metadata": {},
   "source": [
    "Below I've \"inverted\" the scree plot\n",
    "to show the percentage of information retained\n",
    "versus number of dimensions retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001f780-0b5f-4dc8-a246-210dcdbc2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(S)/np.sum(S))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e4ac9-5cd4-4bdb-a951-ef344c90c9d1",
   "metadata": {},
   "source": [
    "Let's retain only 5 dimensions and take a look at the results.\n",
    "\n",
    "First, we'll project our data onto the best 5D approximation of our covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09216d6f-b96b-4ce6-85e0-bf8d6dd41926",
   "metadata": {},
   "outputs": [],
   "source": [
    "ND = 5\n",
    "\n",
    "P = np.dot(np.diagflat(S[0:ND]), Vh[0:ND,:])\n",
    "compressed_data = np.dot(P,data_normalized)\n",
    "compressed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29512285-3e35-485b-acce-de3fff345445",
   "metadata": {},
   "source": [
    "**Explain the shape of `compressed_data` shown above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e8a52-38fd-412b-968e-78795382d984",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26d914-0385-40b6-af68-f5104ec57639",
   "metadata": {},
   "source": [
    "Our `compressed_data` currently lives in a 5D space with the basis determined by SVD.\n",
    "We'd like to bring it back into our original image space and coordinate system.\n",
    "\n",
    "Unfortunately, the matrix `P` is singular, so we can't invert it.\n",
    "Instead, we'll use the \"pseudo-inverse\" as the next best thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f61bb-0b41-46c8-9afb-752c9d191542",
   "metadata": {},
   "outputs": [],
   "source": [
    "UnP = np.linalg.pinv(P)\n",
    "UnP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5107b11-3410-4f30-a62b-0a0dad1c6c6c",
   "metadata": {},
   "source": [
    "Now we'll use our \"unpack\" (`UnP`) matrix to bring our `compressed_data` back into the original input space.\n",
    "\n",
    "Note that we need to undo the data normalization, because that wasn't accounted for by the SVD process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997f118-ed4e-4e63-9801-df6c18cf8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_data = np.dot(UnP,compressed_data)*data_stddev + data_mean\n",
    "unpacked_images = np.reshape(unpacked_data.T, (n_samples, 8, 8))\n",
    "unpacked_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5ae0c-854d-4b0c-a9d0-d5f563315af4",
   "metadata": {},
   "source": [
    "Let's see how well 5 dimensions representated our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9cd2c0-a510-459d-ac99-0e937f2028aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=20, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, unpacked_images, dataset.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"%i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf4812-1a36-4445-916f-37f82d2b697e",
   "metadata": {},
   "source": [
    "**What are your thoughts on these images?  Are 5 dimensions enough?  Why do some numbers appear reasonably clear while others are garbled?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9a1b1-934f-4b8c-adac-c79a6d4758ca",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b7eb0-aa75-463c-aea3-8b5b61363cdf",
   "metadata": {},
   "source": [
    "Take a look at the first unpacked image values versus the original image values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594cf328-f718-4ac4-8aeb-925cd4c5eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unpacked_images[0] =\\n{}'.format(unpacked_images[0]))\n",
    "print('dataset.images[0] = \\n{}'.format(dataset.images[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b508e6-2119-43d6-af65-4c761f7b72e1",
   "metadata": {},
   "source": [
    "Let's try the same steps again, but this time we'll retain all 64 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b041e9-21e2-4136-b060-6b1f8ed3bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ND = 64\n",
    "\n",
    "P = np.dot(np.diagflat(S[0:ND]), Vh[0:ND,:])\n",
    "compressed_data = np.dot(P,data_normalized)\n",
    "compressed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66767e21-106f-4185-87f1-26c221238c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "UnP = np.linalg.pinv(P)\n",
    "\n",
    "unpacked_data = np.dot(UnP,compressed_data)*data_stddev + data_mean\n",
    "unpacked_images = np.reshape(unpacked_data.T, (n_samples, 8, 8))\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=20, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, unpacked_images, dataset.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"%i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dbb686-ba4a-46bf-b5cb-c58ff3cf30bc",
   "metadata": {},
   "source": [
    "For comparison, let's look at the original dataset images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aaef8b-942f-4f08-946e-5f5e07ff454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=20, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, dataset.images, dataset.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"%i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37412914-ae1d-4d3c-8e85-b448b87a1839",
   "metadata": {},
   "source": [
    "**What do you notice?  Why is that the case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350bd27-2602-4ffe-8dd9-22431eb1cd1b",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3595c-cbbd-4a68-b992-6660686f1c7d",
   "metadata": {},
   "source": [
    "Let's compare the unpacked/original image values again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7f295-c5c5-4550-b57e-cb0cdad757c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unpacked_images[0] =\\n{}'.format(unpacked_images[0]))\n",
    "print('dataset.images[0] = \\n{}'.format(dataset.images[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9828c-aec8-4170-bd5b-167c4ba5e813",
   "metadata": {},
   "source": [
    "**How well did 64 dimensions approximate our original dataset?  Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d696f-051b-46f8-9384-d4d72ce8022b",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac35a7-9e1e-4f8d-835e-33a5159ab89a",
   "metadata": {},
   "source": [
    "It would seem that there is some middle ground between 5D and 64D that would serve our purpose.  Let's try to find it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7819ce-778b-4f1f-92bc-cbfefd2c00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ND in range(4,65,4):    \n",
    "    P = np.dot(np.diagflat(S[0:ND]), Vh[0:ND,:])\n",
    "    compressed_data = np.dot(P,data_normalized)\n",
    "    UnP = np.linalg.pinv(P)\n",
    "\n",
    "    unpacked_data = np.dot(UnP,compressed_data)*data_stddev + data_mean\n",
    "    unpacked_images = np.reshape(unpacked_data.T, (n_samples, 8, 8))\n",
    "\n",
    "    _, axes = plt.subplots(nrows=1, ncols=20, figsize=(10, 3))\n",
    "    for ax, image, label in zip(axes, unpacked_images, dataset.target):\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "        ax.set_title(\"%i\" % ND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edf634-fe0b-44a7-b37e-fd4ea3dd016c",
   "metadata": {},
   "source": [
    "**Based on the above plots, how many dimensions would you retain?  How did you come to that conclusion?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b6fb4-abb9-4d86-98ed-5a6954462762",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb45ef-ba56-412d-bda4-2b2c80116bb9",
   "metadata": {},
   "source": [
    "**What did you think of this exercise?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfc220-a85b-4c84-b78b-41d44fc62222",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a814e7-1ad3-4f5d-9524-7afeb3af3957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
